{"pages":[],"posts":[{"title":"","text":"1 Define the term ‘Iron Triangle’ used in the project life-cycle? The Iron Triangle refers to the constraints related to a project. The three sides include: the time, the cost, and the scope &amp; resources a project requires. The change on one constrain will certainly have effects on the other two. For example, if we cut down the expenses for the project, the scope will shrink, and the time required to complete the project might delay. 2 Explain the main reasons to avoid using the action ‘will’ as language requirements? The use of ‘will’ needs to be avoided for legal reasons. A well-formed requirement statement should be measurable and bounded and should also be verifiable. But the use of 'will' doesn’t give an exact time scope for the context. For example, by saying ‘the website will be built’, it doesn’t guarantee a final product to be verified. 3 Explain the main methods of construction used in the development phase of the project life-cycle? There are three common methods of construction: 1. Building a software from scratch. It is a 0 to 1 process. e.g. building a website from empty .html and .css files. 2. Re-using an existing software, with or without modification, is to adapt templates that were developed by others. e.g. building a website using WIX. 3. Acquisition. This method refers to any open-source projects. e.g. building a website using Microweber.org. ?????? 4 Explain the main purpose of the project planning phase in the project life-cycle? Project planning is to enable the tracking process and increase collaboration by system analysis and design activities. Its main purpose is to determine the scope of the project as well as the objective of the project. For example, if the budget wasn't properly estimated in the planning stage, then the project might overspend money without a desirable outcome. 5 Explain two software models or paradigms used in software development, you can highlight the benefits, and drawbacks of each. 1.Waterfall's advantages include: Easy to understand, Easy to manage, Fewer production issues, and Better budget management. Waterfall's disadvantages include: Not flexible, Doesn’t handle unexpected risks well, Not good for complex/long projects, and Difficult to capture all requirements upfront 2.V-Model's advantages include: Highly disciplined Simple to understand &amp; use Increased quality Accurately Tracks Progress V-Model's disadvantages include: High risk &amp; uncertainty Ill-suited to complex or long projects Doesn’t handle change well - hard to go back. Nothing concrete is created until midway through the process. 6 Explain in which scenarios the Agile model is suitable or not suitable to be used in software development models? Agile is an iterative &amp; incremental Model. It delivers iterations frequently to adapt changes requested from customers. It also reduces waste and empower team ownership. It is suitable for: Small to medium projects Product can be broken into independent functional parts Unclear/dynamic requirements Co-located teams Proactive, collaborative teams It is not suitable for: hierarchy static and routine project large scale projects with individual workers 7 Define the meaning of ‘models and paradigms’ in a software development environment? Models and paradigms describe a structure of procedures used to complete a software development project. It acts like an overall plan of how to plan a project. Different models and paradigms are chosen to fit different project needs. For example, the waterfall model has less advantage in a project that has constantly changing requirements, while having more advantage in a project that is static and low in cost. 8 Explain the following statement ‘how can a software methodology impact the project life-cycle’ ? Companies intergrate software development methodologies expecting a greatly impoved project performance. They play a role in supporting the project team throughout the life cycle to achieve the projects' goals. Each development team should choose the most suitable methodology to apply to the development project. Different methods rotates the project life-cycle in different frequency. 9 Define the term ‘Kanban’, and explain how it can be used in software projects? Kanban is a Japanese term that means 'billboard'. It is a framework in Agile based on a team's need and capacity. It focuses on visualizing the workflow by using a system of cards (Kanban) to represent tasks at different stages of the project. Moving the cards or tasks through the predetermined stages help the team to see their progress and identify where roadblocks might occur. The practices should be strickly followed to ensure its success. 10 Explain the main purposes of using sprint retrospectives in Scrum methodology? Sprint retrospective happens after the review session and the planning session. The main purpose is for the team to inspects itself and create a plan for improvement, such as improving relationships between team members, identifying what worked and what didn't work, reviewing the resource list, and improving the plan for implementation. The goal is to fix one thing at a time and make small, incremental changes from sprint to sprint. Using sprint retrospectives in Scrum methodology allows for rapid development and testing, especially within small teams. 11 Define the term ‘XP’ in Agile methods? 'XP' refers to 'Extreme Programming'. It is a software development methodology that strived for efficiency and customer satisfaction by practicing a set of specific values and rules in its teamwork workflow. It empowers the developers to adapt to the evolving and changing requirements even late in the project life cycle. Its goal is to allow small to mid-sized teams to develop high-quality product efficiently. 12 Explain how would lean software development model eliminate waste? Lean software development is focused on eliminating waste. The term 'waste' in this context refers to anything that is not adding the value to the project. Over-production, delay in process, unclear/changing requirements, partially done work, defects, task switching, extra features... all these above could produce waste. Unnecessary code or functionality Starting more than can be completed. Delay in the software development process. Unclear or constantly changing requirements. Partially done work. Defects and quality issues. Task switching. Each of these wastes should be systematically eliminated to maximize customer value. Keep simplicity is the key. 13 Define the term ‘sprint’ used in the software development model? 'Sprint' is a short period of time wherein a development team works to complete specific tasks, deliverables, or other small schivable targets that breaks down a complex software development project. Sprints typically are done weekly, and do not extend longer than one calendar month. A Sprint is composed of: - Sprint planning: the team gathers to determine how much work to be complete in a sprint. - Daily Scrum: 'stand-up meeting', quick touch points to make sure the team is running on schedule and everyone is involved in problem solving. - Sprint review: after a sprint completes, the project manager hosts a sprint review to demonstrate the complete outputs. - Sprint retrospective: discuss accomplishments and challenges to fix one thing at a time and make small, incremental changes in the next sprint. 14 Explain the main elements of stakeholder register used in the initiation phase in the project life-cycle?1 the main elements: Name, title, position, company and contact information of each stakeholder involved in the project. 2 Reporting relationships between stakeholders and others. 3 The main interests of stakeholders in the project. 4 Requirements 5 Internal External 6 Degree of participation 7 The level of access 8 communication. The main elements of stakeholder register include: - Stakeholder’s identification: Group name, location, contact details - Stakeholder’s assessment information: Role in the project, requirements - Stakeholder’s classification: Internal or external, Classified (based on their influence and impact on the project 15 Describe the main processes used in the project initiation? Project Initiation is to define the process needed to start a new project. It has two main activities: - Identify the stakeholders. A stakeholder can influence the success and the failure of the project. Thus, it is essential to record information such as their role, requirements, contacts...into the Stakeholder Register. - Develop a Project Charter. The project charter is used as the vision, mission, and success criteria of the project. Generally, it would include information such as the Iron Triangle, business value, risks, stakeholders, and so on. 16 Explain the following statement: In our opinion why do you think project planning fails? There are a few reasons that a project planning might fail. - The Planning phase is by Activity rather than Feature. - The Activities don’t finish early. - Lateness is passed down the schedule. - Activities are not independent. - Multitasking causes further delays. - The features are not developed by priority. - Uncertainty is ignored. - Estimates become commitments. 17 Describe how does the estimation concept work in agile methods? Estimation is roughly giving a time frame for how long a task need to be done. It is a relatively quick exercise that can be improved. The main idea of the Agile estimation is asking 'how big' questions, which is focusing on consistency rather than giving specific hours. For example, we can catergorize tasks into 'Small' 'Medium' 'Large', and estimate Small tasks need 3-4 hours, Medium tasks 1-2 days, and Large tasks 5-7 days. Estimate only what is needed to avoid overthinking. 18 Explain the main factors affecting the estimation in agile methods? The main factors affecting the Agile Estimation include: - Wishful Thinking: It means to base the estimates on top of the head rather than reasoning. Data or past experiences should be used as an evidence to each estimation. - Anchoring: It means to overly rely on the first information being seen. The initial piece of information usually dominates the decisions afterwards, but it may not be true. - Planning fallacy: It means to underestimate for being too optimized. Personal bias happens for sure, so it's important to recognize its existence and to avoid it as much as we can. - Cognitive Dissonance: It means to hold two contradictory ideas simultaneously. Ideas should be aligned with each other. 19 Define documentation and explain the main perspective of including documentation in projects? The base material of software crafting is Knowledge, and documents are used for managing them. Knowledge can be gathered from requirements and then transform to the information needed to analysis, architecture, designs, implementations, tests, reports..., and ALL of these must be documented. The three perspectives on documents include: - Familiarization - presenting knowledge to all stakeholders - Education - training the developers with the skills needed - Support - troubleshooting the project 20 Describe the main elements of documentation quality focus? The main elements of documentation quality focus can be classified into three aspects: - Integrity: include everything. The documentation should cover all features, usage modes, and interfaces. The essential questions (what, how, where) should be answered. - Fidelity: The document should be logically organized to line out the actions for the prototype matching with the target step by step. - Suitability: The document should know its targeted audience can access the document. It needs to be consistent and professional, and answer all of the possible questions readers might have. 21 Define the term ‘scheduling’ used in project management, and explain the popular techniques used in scheduling? Scheduling is the process of deciding how to commit resources between a variety of possible tasks. Regardless of methodologies, it's still important to forecast long into the future, make deadlines, and plan resources, which scheduling would do. The three main factors to consider when scheduling a task include its duration, dependencies, and constraints. These factors help to estimate the start and finish dates for each task. Gantt Charts are used to visualize this information. Program Evaluation Review Technique (PERT) is a popular to use while scheduling. It has 3-point estimation approach: - Optimistic approach - O: shorted completion time - Most Likely approach - M: balances good/bad conditions - Pessimistic approach - P: longest completion time 22 Describe the main components of interaction in the governance model. Governance is the management framework within which decisions are made. The IT Governance Model is the coordinated interaction of three levels of components. At the bottom level is the Infrastructure, which includes hardware, software, information...everything that enables the collaboration. After that we go to the middle level, which is the Operating procedures to keep the software up to date. At the top level is the Decision-making structures representing management. 23 Describe three main IT governance and risk framework used in IT systems. IT Governance frameworks are to make sure IT systems are providing needed value and limiting risks. There are three main frameworks. COBIT is the IT governance and control model. CMM, which stands for Capability Maturity Model, is focusing on software engineering. ITIL (IT Infrastructure Library) s focusing on IT Service Management. 24 Define the meaning of ‘risk’ and explain the main steps involved in risk management? 'Risk' can be defined as an uncertain condition with consequences (often negative), or the uncertainty around certain factors of the condition. Risk management involves three main processes. First, identify the risk. It could be legal risks, market risks, environmental risks... as much as possible. It is important to expose and visualize anything that could possibly go wrong, so that the second step could be done: prioritizing and determining what can be managed. That involves risk analysis, evaluation, and categorization. The last step is mitigating the effects of potential risks. Not all risks can be eliminated, so we should strive for keeping the risks at their minimal harm. 25 Define the term ‘risk register’ and describe how do ‘risk registers’ managing risks? 'Risk Register' is basically a document for reporting risks. It should include what the risks are, such basic information as the description, the date identified, the impact, the severity, the probability, the priority, ways to manage them, and so on. Newly discovered risks would continually be documented on the register throughout the project. Actions then will be taken to address these identified risks. 26 Define the term ‘governance’, and explain how governance models work in Agile? Governance is the management framework within which decisions are made. In Agile projects, it is important to understand what your organization requires as well as to engage with stakeholders. Governance should minimize the unnecessary work, and make sure the project develop environment is **safe and controlled**. 27 Explain how does ‘retrospective’ work in Agile? Retrospective is a meeting after scrum. It's a time set aside to reflect on past events for the purpose of leaning and improving in the future. In Agile, it provides a safe space for the team to identify what didn't work and what can work better. It embraces a positive spirit to discuss freely for continuous improvements. After a retrospective, actions will be assigned and deadlines will be set. 28 Explain three reasons why quality assurance is essential for agile projects? Quality Assurance is essential because we want our products are matching all the requirements while following all the policies and procedures. To meet that goal, proactive analysis of processes, methods, and outputs is performed. Quality assurance is taken place throughout the project life-cycle. 1. It happens throughout the life cycle 2. It can actively analyze the process, method and product to ensure that the product produced has the required quality 3. It can ensure compliance with policies and procedures. 29 Briefly explain each of the main steps used in Pre-Mortem (failure analysis)? Pre-Mortem, or failure analysis is to identify potential failures, which provides the team an opportunity to be proactive. There are four main steps in Pre-Mortem. - Imagine the failure. In this step, the team brainstorm the possible issues. The facilitator can suggest scenarios. For example, an UX failure could happen when choosing font, too complicated, not enough information provided... - Generate reasons for the failure. In this step, the team members work independently to list possible reasons that causes the failure. For example: loss of funding, loss of key staff, inadequate feedbacks... - Consolidate the list. In this step, the whole team work together to put the list and its according actions into priority. - Revisit the plan. In this step, the Project Owner would approve or add actions on top of the priority, which will add to the requirements for people to work on. Don't expand too much on it so people don't scare away. Keep it simple and creative. 30 Describe the main benefits for ‘Theory of Constraints’ in agile projects? The Theory of Constraints says that all systems have multiple activities linked together. It's impossible to have everything at once, so there is always one activity which acts as the constraints of others, or 'bottleneck'. An example would be the Iron Triangle. The TOC focuses on systematically eliminating the 'bottleneck' until it is no longer a limiting factor. Increased **profit** **Fast** improvement Improved **capacity** Reduced **lead times** Reduced **WIP**","link":"/2021/11/23/Short%20Answer/"},{"title":"hello, world","text":"","link":"/2021/12/13/hello-world/"},{"title":"","text":"目录 Table of Contents 什么是机器学习 What is Machine Learning (ML) 实际应用领域 Real-World Applications 机器学习的类别 Types of ML problems Supervised Learning Classification Regression Unsupervised Learning Clustering 如何选择合适的算法 How to choose the appropriate algorithm 适用机器学习的情景 When to appropriately use ML Introduction What is Machine Learning? Machine learning teaches computers to do what comes naturally to humans and animals: learn from experience. Machine learning algorithms use computational methods to “learn” information directly from data without relying on a predetermined equation as a model. The algorithms adaptively improve their performance as the number of samples available for learning increases. 机器学习教计算机从过往的经验中总结和学习。机器学习算法使用计算方法直接从数据中“学习”信息，而不依赖预先确定的方程作为模型。随着可用于学习的样本数量的增加，这些算法自适应地提高其性能。 More Data, More Questions, Better Answers Machine learning algorithms find natural patterns in data that generate insight and help you make better decisions and predictions. They are used every day to make critical decisions in medical diagnosis, stock trading, energy load forecasting, and more. Media sites rely on machine learning to sift through millions of options to give you song or movie recommendations. Retailers use it to gain insight into their customers’ purchasing behavior. （Real-World Applications）With the rise in big data, machine learning has become particularly important for solving problems in areas like these: Computational finance, for credit scoring and algorithmic trading Image processing and computer vision, for face recognition, motion detection, and object detection Computational biology, for tumor detection, drug discovery, and DNA sequencing Energy production, for price and load forecasting Automotive, aerospace, and manufacturing, for predictive maintenance Natural language processing How Machine Learning Works Machine learning uses two types of techniques: Supervised learning: trains a model on known input and output data so that it can predict future outputs Develop predictive model based on both input and output data Classification Support Vector Machines Discriminal Analysis Naive Bayes Nearest Neighbor Regression Linear Regression, GLM SVR, GPR Ensemble Methods Decision Trees Neutural Networks Choose supervised learning if you need to train a model to make a prediction for example, the future value of a continuous variable, such as temperature or a stock price, or a classification—for example, identify makes of cars from webcam video footage. Unsupervised learning: finds hidden patterns or intrinsic structures in input data. Group and interpret data based on input data Clustering K-Means, K-Medoids, Fuzzy C-Means Hierarchical Gaussian Mixture Neutral Networks Hidden Markov Model Choose unsupervised learning if you need to explore your data and want to train a model to find a good internal representation, such as splitting data up into clusters. Supervised Learning The aim of supervised machine learning is to build a model that makes predictions based on evidence in the presence of uncertainty. A supervised learning algorithm takes a known set of input data and known responses to the data (output) and trains a model to generate reasonable predictions for the response to new data. Supervised learning uses classification and regression techniques to develop predictive models. Classification techniques predict discrete responses—for example, whether an email is genuine or spam, or whether a tumor is cancerous or benign. Classification models classify input data into categories. Typical applications include medical imaging, speech recognition, and credit scoring. Regression techniques predict continuous responses— for example, changes in temperature or fluctuations in power demand. Typical applications include electricity load forecasting and algorithmic trading. Using Supervised Learning to Predict Heart Attacks Suppose clinicians want to predict whether someone will have a heart attack within a year. They have data on previous patients, including age, weight, height, and blood pressure. They know whether the previous patients had heart attacks within a year. So the problem is combining the existing data into a model that can predict whether a new person will have a heart attack within a year. Unsupervised Learning Unsupervised learning finds hidden patterns or intrinsic structures in data. It is used to draw inferences from datasets consisting of input data without labeled responses. Clustering is the most common unsupervised learning technique. It is used for exploratory data analysis to find hidden patterns or groupings in data. Applications for clustering include gene sequence analysis, market research, and object recognition. How Do You Decide Which Algorithm to Use? Choosing the right algorithm can seem overwhelming—there are dozens of supervised and unsupervised machine learning algorithms, and each takes a different approach to learning. There is no best method or one size fits all. Finding the right algorithm is partly just trial and error—even highly experienced data scientists can’t tell whether an algorithm will work without trying it out. But algorithm selection also depends on the size and type of data you’re working with, the insights you want to get from the data, and how those insights will be used. When Should You Use Machine Learning? Consider using machine learning when you have a complex task or problem involving a large amount of data and lots of variables, but no existing formula or equation. For example, machine learning is a good option if you need to handle situations like these: Hand-written rules and equations are too complex—as in face recognition and speech recognition. The rules of a task are constantly changing—as in fraud detection from transaction records. The nature of the data keeps changing, and the program needs to adapt—as in automated trading, energy demand forecasting, and predicting shopping trends. Real-World Examples …P14 for more Getting Started Rarely a Straight Line With machine learning there’s rarely a straight line from start to finish—you’ll find yourself constantly iterating and trying different ideas and approaches. This chapter describes a systematic machine learning workflow, highlighting some key decision points along the way. Machine Learning Challenges Most machine learning challenges relate to handling your data and finding the right model. Data comes in all shapes and sizes. Real-world datasets can be messy, incomplete, and in a variety of formats. You might just have simple numeric data. But sometimes you’re combining several different data types, such as sensor signals, text, and streaming images from a camera. Preprocessing your data might require specialized knowledge and tools. For example, to select features to train an object detection algorithm requires specialized knowledge of image processing. Different types of data require different approaches to preprocessing. It takes time to find the best model to fit the data. Choosing the right model is a balancing act. Highly flexible models tend to overfit data by modeling minor variations that could be noise. On the other hand, simple models may assume too much. There are always tradeoffs between model speed, accuracy, and complexity. Sounds daunting? Don’t be discouraged. Remember that trial and error is at the core of machine learning—if one approach or algorithm doesn’t work, you simply try another. But a systematic workflow will help you get off to a smooth start. Questions to Consider Before You Start Every machine learning workflow begins with three questions: What kind of data are you working with? What insights do you want to get from it? How and where will those insights be applied? Your answers to these questions help you decide whether to use supervised or unsupervised learning. Workflow at a Glance ACCESS and load the data. PREPROCESS the data. DERIVE features using the preprocessed data. TRAIN models using the features derived in step 3. ITERATE to find the best model. INTEGRATE the best-trained model into a production system. Training a Model to Classify Physical Activities In the next sections we’ll look at the steps in more detail, using a health monitoring app for illustration. The entire workflow will be completed in MATLAB. This example is based on a cell phone health-monitoring app. The input consists of three-axial sensor data from the phone’s accelerometer and gyroscope. The responses, (or output), are the activities performed–walking, standing, running, climbing stairs, or lying down. We want to use the input data to train a classification model to identify these activities. Since our goal is classification, we’ll be applying supervised learning. The trained model (or classifier) will be integrated into an app to help users track their activity levels throughout the day. Step One: Load the Data To load data from the accelerometer and gyroscope we do the following: Sit down holding the phone, log data from the phone, and store it in a text file labeled “Sitting.” Stand up holding the phone, log data from the phone, and store it in a second text file labeled “Standing.” Repeat the steps until we have data for each activity we want to classify. We store the labeled data sets in a text file. A flat file format such as text or CSV is easy to work with and makes it straightforward to import data. Machine learning algorithms aren’t smart enough to tell the difference between noise and valuable information. Before using the data for training, we need to make sure it’s clean and complete. Step Two: Preprocess the Data Look for outliers–data points that lie outside the rest of the data. This is what we do to process the imported data in MATLAB and plot each labeled set. We must decide whether the outliers can be ignored or whether they indicate a phenomenon that the model should account for. In our example, they can safely be ignored (it turns out that we moved unintentionally while recording the data). Check for missing values (perhaps we lost data because the connection dropped during recording). We could simply ignore the missing values, but this will reduce the size of the data set. Alternatively, we could substitute approximations for the missing values by interpolating or using comparable data from another sample. Remove gravitational effects from the accelerometer data so that our algorithm will focus on the movement of the subject, not the movement of the phone. A simple high- pass filter such as a biquad filter is commonly used for this. Divide the data into two sets. We save part of the data for testing (the test set) and use the rest (the training set) to build models. This is referred to as holdout, and is a useful cross- validation technique. Step Three: Derive Features Deriving features (also known as feature engineering or feature extraction) is one of the most important parts of machine learning. It turns raw data into information that a machine learning algorithm can use. For the activity tracker, we want to extract features that capture the frequency content of the accelerometer data. These features will help the algorithm distinguish between walking (low frequency) and running (high frequency). We create a new table that includes the selected features. Use feature selection to: Improve the accuracy of a machine learning algorithm Boost model performance for high-dimensional data sets Improve model interpretability Prevent overfitting Step Four: Build and Train the Model The number of features that you could derive is limited only by your imagination. However, there are a lot of techniques commonly used for different types of data. Sensor data Feature Selection Task: Extract signal properties from raw sensor data to create higher-level information Techniques: Peak analysis perform an fft and identify dominant frequencies Pulse and transition metrics derive signal characteristics such as rise time, fall time, and settling time Spectral measurements plot signal power, bandwidth, mean frequency, and median frequency Image and video data Feature Selection Task: Extract features such as edge locations, resolution, and color Techniques: Bag of visual words create a histogram of local image features, such as edges, corners, and blobs Histogram of oriented gradients (HOG) create a histogram of local gradient directions Minimum eigenvalue algorithm detect corner locations in images Edge detection identify points where the degree of brightness changes sharply Transactional data Feature Selection Task: Calculate derived values that enhance the information in the data Techniques： Timestamp decomposition break timestamps down into components such as day and month Aggregate value calculation create higher-level features such as the total number of times a particular event occurred When building a model, it’s a good idea to start with something simple; it will be faster to run and easier to interpret. To see how well it performs, we plot the confusion matrix, a table that compares the classifications made by the model with the actual class labels that we created in step 1. The confusion matrix shows that our model is having trouble distinguishing between dancing and running. Maybe a decision tree doesn’t work for this type of data. We’ll try a few different algorithms. We start with a K-nearest neighbors (KNN), a simple algorithm that stores all the training data, compares new points to the training data, and returns the most frequent class of the “K” nearest points. That gives us 98% accuracy compared to 94.1% for the simple decision tree. The confusion matrix looks better, too: However, KNNs take a considerable amount of memory to run, since they require all the training data to make a prediction. We try a linear discriminant model, but that doesn’t improve the results. Finally, we try a multiclass support vector machine (SVM). The SVM does very well—we now get 99% accuracy: We achieved our goal by iterating on the model and trying different algorithms. If our classifier still couldn’t reliably differentiate between dancing and running, we’d look into ways to improve the model. Step Five: Improve the Model Improving a model can take two different directions: make the model simpler or add complexity. Simplify First, we look for opportunities to reduce the number of features. Popular feature reduction techniques include: Correlation matrix – shows the relationship between variables, so that variables (or features) that are not highly correlated can be removed. Principal component analysis (PCA) – eliminates redundancy by finding a combination of features that captures key distinctions between the original features and brings out strong patterns in the dataset. Sequential feature reduction – reduces features iteratively on the model until there is no improvement in performance. Next, we look at ways to reduce the model itself. We can do this by Pruning branches from a decision tree Removing learners from an ensemble A good model includes only the features with the most predictive power. A simple model that generalizes well is better than a complex model that may not generalize or train well to new data. In machine learning, as in many other computational processes, simplifying the model makes it easier to understand, more robust, and more computationally efficient. Add Complexity If our model can’t differentiate dancing from running because it is over-generalizing, then we need find ways to make it more fine-tuned. To do this we can either: Use model combination – merge multiple simpler models into a larger model that is better able to represent the trends in the data than any of the simpler models could on their own. Add more data sources – look at the gyroscope data as well as the acceleromter data. The gyroscope records the orientation of the cell phone during activity. This data might provide unique signatures for the different activities; for example, there might be a combination of acceleration and rotation that’s unique to running. Once we’ve adjusted the model, we validate its performance on the test data that we set aside during preprocessing. If the model can reliably classify activities on the test data, we’re ready to move it to the phone and start tracking. Applying Unsupervised Learning When to Consider Unsupervised Learning Unsupervised learning is useful when you want to explore your data but don’t yet have a specific goal or are not sure what information the data contains. It’s also a good way to reduce the dimensions of your data. Unsupervised Learning Techniques As we saw in section 1, most unsupervised learning techniques are a form of cluster analysis. In cluster analysis, data is partitioned into groups based on some measure of similarity or shared characteristic. Clusters are formed so that objects in the same cluster are very similar and objects in different clusters are very distinct. Clustering algorithms fall into two broad groups: Hard clustering, where each data point belongs to only one cluster Soft clustering, where each data point can belong to more than one cluster You can use hard or soft clustering techniques if you already know the possible data groupings. If you don’t yet know how the data might be grouped: Use self-organizing feature maps or hierarchical clustering to look for possible structures in the data. Use cluster evaluation to look for the “best” number of groups for a given clustering algorithm. Common Hard Clustering Algorithms k-Means How it Works Partitions data into k number of mutually exclusive clusters. How well a point fits into a cluster is determined by the distance from that point to the cluster’s center. Best Used… When the number of clusters is known For fast clustering of large data sets Result: Cluster centers Example: Using k-Means Clustering to Site Cell Phone Towers A cell phone company wants to know the number and placement of cell phone towers that will provide the most reliable service. For optimal signal reception, the towers must be located within clusters of people. The workflow begins with an initial guess at the number of clusters that will be needed. To evaluate this guess, the engineers compare service with three towers and four towers to see how well they’re able to cluster for each scenario (in other words, how well the towers provide service). A phone can only talk to one tower at a time, so this is a hard clustering problem. The team uses k-means clustering because k-means treats each observation in the data as an object having a location in space. It finds a partition in which objects within each cluster are as close to each other as possible and as far from objects in other clusters as possible. After running the algorithm, the team can accurately determine the results of partitioning the data into three and four clusters. k-Medoids How It Works Similar to k-means, but with the requirement that the cluster centers coincide with points in the data. Best Used… When the number of clusters is known For fast clustering of categorical data To scale to large data sets Result: Cluster centers that coincide with data points Hierarchical Clustering How it Works Produces nested sets of clusters by analyzing similarities between pairs of points and grouping objects into a binary, hierarchical tree. Best Used… When you don’t know in advance how many clusters are in your data You want visualization to guide your selection Result: Dendrogram showing the hierarchical relationship between clusters Self-Organizing Map How It Works Neural-network based clustering that transforms a dataset into a topology-preserving 2D map. Best Used… To visualize high-dimensional data in 2D or 3D To deduce the dimensionality of data by preserving its topology (shape) Result: Lower-dimensional (typically 2D) representation Common Soft Clustering Algorithms Fuzzy c-Means How it Works Partition-based clustering when data points may belong to more than one cluster. Best Used… When the number of clusters is known For pattern recognition When clusters overlap Result: Cluster centers (similar to k-means) but with fuzziness so that points may belong to more than one cluster Example: Using Fuzzy c-Means Clustering to Analyze Gene Expression Data A team of biologists is analyzing gene expression data from microarrays to better understand the genes involved in normal and abnormal cell division. (A gene is said to be “expressed” if it is actively involved in a cellular function such as protein production.) The microarray contains expression data from two tissue samples. The researchers want to compare the samples to determine whether certain patterns of gene expression are implicated in cancer proliferation. After preprocessing the data to remove noise, they cluster the data. Because the same genes can be involved in several biological processes, no single gene is likely to belong to one cluster only. The researchers apply a fuzzy c-means algorithm to the data. They then visualize the clusters to identify groups of genes that behave in a similar way. Gaussian Mixture Model How It Works Partition-based clustering where data points come from different multivariate normal distributions with certain probabilities. Best Used… When a data point might belong to more than one cluster When clusters have different sizes and correlation structures within them Result: A model of Gaussian distributions that give probabilities of a point being in a cluster Improving Models with Dimensionality Reduction Machine learning is an effective method for finding patterns in number of features, or dimensionality. As datasets get bigger, you frequently need to reduce the big datasets. But bigger data brings added complexity. Example: EEG Data Reduction Suppose you have electroencephalogram (EEG) data that captures electrical activity of the brain, and you want to use this data to predict a future seizure. The data was captured using dozens of leads, each corresponding to a variable in your original dataset. Each of these variables contains noise. To make your prediction algorithm more robust, you use dimensionality reduction techniques to derive a smaller number of features. Because these features are calculated from multiple sensors, they will be less susceptible to noise in an individual sensor than would be the case if you used the raw data directly. Common Dimensionality Reduction Techniques The three most commonly used dimensionality reduction techniques are: Principal Component Analysis (PCA)— performs a linear transformation on the data so that most of the variance or information in your high-dimensional dataset is captured by the first few principal components. The first principal component will capture the most variance, followed by the second principal component, and so on. Using Principal Component Analysis In datasets with many variables, groups of variables often move together. PCA takes advantage of this redundancy of information by generating new variables via linear combinations of the original variables so that a small number of new variables captures most of the information. Each principal component is a linear combination of the original variables. Because all the principal components are orthogonal to each other, there is no redundant information. Example: Engine Health Monitoring You have a dataset that includes measurements for different sensors on an engine (temperatures, pressures, emissions, and so on). While much of the data comes from a healthy engine, the sensors have also captured data from the engine when it needs maintenance. You cannot see any obvious abnormalities by looking at any individual sensor. However, by applying PCA, you can transform this data so that most variations in the sensor measurements are captured by a small number of principal components. It is easier to distinguish between a healthy and unhealthy engine by inspecting these principal components than by looking at the raw sensor data. Factor Analysis— identifies underlying correlations between variables in your dataset to provide a representation in terms of a smaller number of unobserved latent, or common, factors. Using Factor Analysis Your dataset might contain measured variables that overlap, meaning that they are dependent on one another. Factor analysis lets you fit a model to multivariate data to estimate this sort of interdependence. In a factor analysis model, the measured variables depend on a smaller number of unobserved (latent) factors. Because each factor might affect several variables, it is known as a common factor. Each variable is assumed to be dependent on a linear combination of the common factors. Example: Tracking Stock Price Variation Over the course of 100 weeks, the percent change in stock prices has been recorded for ten companies. Of these ten, four are technology companies, three are financial, and a further three are retail. It seems reasonable to assume that the stock prices for companies in the same sector will vary together as economic conditions change. Factor analysis can provide quantitative evidence to support this premise. Nonnegative Matrix Factorization— used when model terms must represent nonnegative quantities, such as physical quantities. Using Nonnegative Matrix Factorization This dimension reduction technique is based on a low-rank approximation of the feature space. In addition to reducing the number of features, it guarantees that the features are nonnegative, producing models that respect features such as the nonnegativity of physical quantities. Example: Text Mining Suppose you want to explore variations in vocabulary and style among several web pages. You create a matrix where each row corresponds to an individual web page and each column corresponds to a word (“the”,”a”,”we”, and so on). The data will be the number of times a particular word occurs on a particular page. Since there more than a million words in the English language, you apply nonnegative matrix factorization to create an arbitrary number of features that represent higher-level concepts rather than individual words. These concepts make it easier to distinguish between, say, news, educational content, and online retail content. Next Steps In this section we took a closer look at hard and soft clustering algorithms for unsupervised learning, offered some tips on selecting the right algorithm for your data, and showed how reducing the number of features in your dataset improves model performance. As for your next steps: Unsupervised learning might be your end goal. For example, if you are doing market research and want to segment consumer groups to target based on web site behavior, a clustering algorithm will almost certainly give you the results you’re looking for. On the other hand, you might want to use unsupervised learning as a preprocessing step for supervised learning. For example, apply clustering techniques to derive a smaller number of features, and then use those features as inputs for training a classifier. In section 4 we’ll explore supervised learning algorithms and techniques, and see how to improve models with feature selection, feature reduction, and parameter tuning. Applying Supervised Learning When to Consider Supervised Learning A supervised learning algorithm takes a known set of input data (the training set) and known responses to the data (output), and trains a model to generate reasonable predictions for the response to new input data. Use supervised learning if you have existing data for the output you are trying to predict. Supervised Learning Techniques All supervised learning techniques are a form of classification or regression. Classification techniques predict discrete responses—for example, whether an email is genuine or spam, or whether a tumor is small, medium, or large. Classification models are trained to classify data into categories. Applications include medical imaging, speech recognition, and credit scoring. Can your data be tagged or categorized? If your data can be separated into specific groups or classes, use classification algorithms. Regression techniques predict continuous responses—for example, changes in temperature or fluctuations in electricity demand. Applications include forecasting stock prices, handwriting recognition, and acoustic signal processing. Working with a data range? If the nature of your response is a real number—such as temperature, or the time until failure for a piece of equipment—use regression techniques. Selecting the Right Algorithm As we saw in section 1, selecting a machine learning algorithm is a process of trial and error. It’s also a trade-off between specific characteristics of the algorithms, such as: Speed of training Memory usage Predictive accuracy on new data Transparency or interpretability (how easily you can understand the reasons an algorithm makes its predictions) Using larger training datasets often yield models that generalize well for new data. Let’s take a closer look at the most commonly used classification and regression algorithms. Binary vs. Multiclass Classification When you are working on a classification problem, begin by determining whether the problem is binary or multiclass. In a binary classification problem, a single training or test item (instance) can only be divided into two classes—for example, if you want to determine whether an email is genuine or spam. In a multiclass classification problem, it can be divided into more than two—for example, if you want to train a model to classify an image as a dog, cat, or other animal. Bear in mind that a multiclass classification problem is generally more challenging because it requires a more complex model. Certain algorithms (for example, logistic regression) are designed specifically for binary classification problems. During training, these algorithms tend to be more efficient than multiclass algorithms. Common Classification Algorithms Logistic Regression How it Works Fits a model that can predict the probability of a binary response belonging to one class or the other. Because of its simplicity, logistic regression is commonly used as a starting point for binary classification problems. Best Used… When data can be clearly separated by a single, linear boundary As a baseline for evaluating more complex classification methods k Nearest Neighbor (kNN) How it Works kNN categorizes objects based on the classes of their nearest neighbors in the dataset. kNN predictions assume that objects near each other are similar. Distance metrics, such as Euclidean, city block, cosine, and Chebychev, are used to find the nearest neighbor. Best Used… When you need a simple algorithm to establish benchmark learning rules When memory usage of the trained model is a lesser concern When prediction speed of the trained model is a lesser concern Support Vector Machine (SVM) How It Works Classifies data by finding the linear decision boundary (hyperplane) that separates all data points of one class from those of the other class. The best hyperplane for an SVM is the one with the largest margin between the two classes, when the data is linearly separable. If the data is not linearly separable, a loss function is used to penalize points on the wrong side of the hyperplane. SVMs sometimes use a kernel transform to transform nonlinearly separable data into higher dimensions where a linear decision boundary can be found. Best Used… For data that has exactly two classes (you can also use it for multiclass classification with a technique called error-correcting output codes) For high-dimensional, nonlinearly separable data When you need a classifier that’s simple, easy to interpret, and accurate Neural Network How it Works Inspired by the human brain, a neural network consists of highly connected networks of neurons that relate the inputs to the desired outputs. The network is trained by iteratively modifying the strengths of the connections so that given inputs map to the correct response. Best Used… For modeling highly nonlinear systems When data is available incrementally and you wish to constantly update the model When there could be unexpected changes in your input data When model interpretability is not a key concern Naïve Bayes How It Works A naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. It classifies new data based on the highest probability of its belonging to a particular class. Best Used… For a small dataset containing many parameters When you need a classifier that’s easy to interpret When the model will encounter scenarios that weren’t in the training data, as is the case with many financial and medical applications Discriminant Analysis How It Works Discriminant analysis classifies data by finding linear combinations of features. Discriminant analysis assumes that different classes generate data based on Gaussian distributions. Training a discriminant analysis model involves finding the parameters for a Gaussian distribution for each class. The distribution parameters are used to calculate boundaries, which can be linear or quadratic functions. These boundaries are used to determine the class of new data. Best Used… When you need a simple model that is easy to interpret When memory usage during training is a concern When you need a model that is fast to predict Decision Tree How it Works A decision tree lets you predict responses to data by following the decisions in the tree from the root (beginning) down to a leaf node. A tree consists of branching conditions where the value of a predictor is compared to a trained weight. The number of branches and the values of weights are determined in the training process. Additional modification, or pruning, may be used to simplify the model. Best Used… When you need an algorithm that is easy to interpret and fast to fit To minimize memory usage When high predictive accuracy is not a requirement Bagged and Boosted Decision Trees How They Work In these ensemble methods, several “weaker” decision trees are combined into a “stronger” ensemble. A bagged decision tree consists of trees that are trained independently on data that is bootstrapped from the input data. Boosting involves creating a strong learner by iteratively adding “weak” learners and adjusting the weight of each weak learner to focus on misclassified examples. Best Used… When predictors are categorical (discrete) or behave nonlinearly When the time taken to train a model is less of a concern Common Classification Algorithms Example Example: Predictive Maintenance for Manufacturing Equipment A plastic production plant delivers about 18 million tons of plastic and thin film products annually. The plant’s 900 workers operate 24 hours a day, 365 days a year. To minimize machine failures and maximize plant efficiency, engineers develop a health monitoring and predictive maintenance application that uses advanced statistics and machine learning algorithms to identify potential issues with the machines so that operators can take corrective action and prevent serious problems from occurring. After collecting, cleaning, and logging data from all the machines in the plant, the engineers evaluate several machine learning techniques, including neural networks, k-nearest neighbors, bagged decision trees, and support vector machines (SVMs). For each technique, they train a classification model using the logged machine data and then test the model’s ability to predict machine problems. The tests show that an ensemble of bagged decision trees is the most accurate model for predicting the production quality. Common Regression Algorithms Linear Regression How it Works Linear regression is a statistical modeling technique used to describe a continuous response variable as a linear function of one or more predictor variables. Because linear regression models are simple to interpret and easy to train, they are often the first model to be fitted to a new dataset. Best Used… When you need an algorithm that is easy to interpret and fast to fit As a baseline for evaluating other, more complex, regression models Nonlinear Regression How It Works Nonlinear regression is a statistical modeling technique that helps describe nonlinear relationships in experimental data. Nonlinear regression models are generally assumed to be parametric, where the model is described as a nonlinear equation. “Nonlinear” refers to a fit function that is a nonlinear function of the parameters. For example, if the fitting parameters are b0, b1, and b2: the equation y = b0+b1x+b2x2 is a linear function of the fitting parameters, whereas y = (b0xb1)/(x+b2) is a nonlinear function of the fitting parameters. Best Used… When data has strong nonlinear trends and cannot be easily transformed into a linear space For fitting custom models to data Gaussian Process Regression Model How it Works Gaussian process regression (GPR) models are nonparametric models that are used for predicting the value of a continuous response variable. They are widely used in the field of spatial analysis for interpolation in the presence of uncertainty. GPR is also referred to as Kriging. Best Used… For interpolating spatial data, such as hydrogeological data for the distribution of ground water As a surrogate model to facilitate optimization of complex designs such as automotive engines SVM Regression How It Works SVM regression algorithms work like SVM classification algorithms, but are modified to be able to predict a continuous response. Instead of finding a hyperplane that separates data, SVM regression algorithms find a model that deviates from the measured data by a value no greater than a small amount, with parameter values that are as small as possible (to minimize sensitivity to error). Best Used… For high-dimensional data (where there will be a large number of predictor variables) Generalized Linear Model How it Works A generalized linear model is a special case of nonlinear models that uses linear methods. It involves fitting a linear combination of the inputs to a nonlinear function (the link function) of the outputs. Best Used… When the response variables have nonnormal distributions, such as a response variable that is always expected to be positive Regression Tree How It Works Decision trees for regression are similar to decision trees for classification, but they are modified to be able to predict continuous responses. Best Used… When predictors are categorical (discrete) or behave nonlinearly Common Regression Algorithms Example Example: Forecasting Energy Load Utility analysts at a large gas and electricity company developed models that predict energy demand for the following day. The models enable power grid operators to optimize resources and schedule power plant generation. Each model accesses a central database for historical power consumption and price data, weather forecasts, and parameters for each power plant, including maximum power out, efficiency, costs, and all the operation constraints that influence the plant dispatch. Analysts looked for a model that provided a low mean absolute percent error (MAPE) to the testing data set. After trying several different types of regression models, it was determined that neural networks provided the lowest MAPE due to their ability to capture the nonlinear behavior of the system. Improving Models Improving a model means increasing its accuracy and predictive power and preventing overfitting (when the model cannot distinguish between data and noise). Model improvement involves feature engineering (feature selection and transformation) and hyperparameter tuning. Feature Selection: Identifying the most relevant features, or variables, that provide the best predictive power in modeling your data. This could mean adding variables to the model or removing variables that do not improve model performance. Feature selection is one of the most important tasks in machine learning. It’s especially useful when you’re dealing with high- dimensional data or when your dataset contains a large number of features and a limited number of observations. Reducing features also saves storage and computation time and makes your results easier to understand. Common feature selection techniques include: Stepwise regression: Sequentially adding or removing features until there is no improvement in prediction accuracy. Sequential feature selection: Iteratively adding or removing predictor variables and evaluating the effect of each change on the performance of the model. Regularization: Using shrinkage estimators to remove redundant features by reducing their weights (coefficients) to zero. Neighborhood component analysis (NCA): Finding the weight each feature has in predicting the output, so that features with lower weights can be discarded. Feature Transformation: Turning existing features into new features using techniques such as principal component analysis, nonnegative matrix factorization, and factor analysis. Feature transformation is a form of dimensionality reduction. As we saw in section 3, the three most commonly used dimensionality reduction techniques are: Principal component analysis (PCA): Performs a linear transformation on the data so that most of the variance or information in your high-dimensional dataset is captured by the first few principal components. The first principal component will capture the most variance, followed by the second principal component, and so on. Nonnegative matrix factorization: Used when model terms must represent nonnegative quantities, such as physical quantities. Factor analysis: Identifies underlying correlations between variables in your dataset to provide a representation in terms of a smaller number of unobserved latent factors, or common factors. Hyperparameter tuning: The process of identifying the set of parameters that provides the best model. Hyperparameters control how a machine learning algorithm fits the model to the data. Like many machine learning tasks, parameter tuning is an iterative process. You begin by setting parameters based on a “best guess” of the outcome. Your goal is to find the “best possible” values— those that yield the best model. As you adjust parameters and model performance begins to improve, you see which parameter settings are effective and which still require tuning. Three common parameter tuning methods are: Bayesian optimization Grid search Gradient-based optimization","link":"/2021/12/21/ML_final/"}],"tags":[],"categories":[]}